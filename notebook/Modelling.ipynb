{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Dataset from Drive ==========\n",
    "# from google.colab import drive\n",
    "# drive.mount('/data/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dir, sub_dirs, _ in os.walk(\"./data\"):\n",
    "    class_names = sub_dirs\n",
    "    break\n",
    "\n",
    "json.dump({\"classes\": class_names}, open(\"classes.json\", \"w\"))\n",
    "n_classes = len(class_names)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Dataset loading ==============\n",
    "\n",
    "directory = \"./data\"\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=class_names,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=120,\n",
    "    validation_split=0.30,\n",
    "    subset=\"training\",\n",
    "    verbose=True)\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=class_names,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=120,\n",
    "    validation_split=0.30,\n",
    "    subset=\"validation\",\n",
    "    verbose=True)\n",
    "\n",
    "# ========== Data Preprocessing ==============\n",
    "def image_rescaling(img, y):\n",
    "    img = img * (1./255)\n",
    "    return img, y\n",
    "\n",
    "def onehot_encode(img, y):\n",
    "    y = tf.one_hot(y, n_classes)\n",
    "    return img, y\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(image_rescaling)\n",
    "train_dataset = train_dataset.map(onehot_encode)\n",
    "\n",
    "validation_dataset = validation_dataset.map(image_rescaling)\n",
    "validation_dataset = validation_dataset.map(onehot_encode)\n",
    "\n",
    "# =========== Prefetchig to efficiently load the dataset in memory =========\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, y in train_dataset:\n",
    "    print(\"img:\", plt.imshow(img[0].numpy()))\n",
    "    print(\"target:\",y[0])\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Input Layer\n",
    "    image_input = tf.keras.Input(shape=(128, 128, 3), name='image_input')\n",
    "    # Convolution Layers\n",
    "    x = tf.keras.layers.Conv2D(16, 3, activation=\"relu\")(image_input)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    ## Fully Connected Layers\n",
    "    features_output = tf.keras.layers.Flatten()(x)\n",
    "    x =  tf.keras.layers.Dense(128, activation=\"relu\")(features_output)\n",
    "    x =  tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    output =  tf.keras.layers.Dense(n_classes, name=\"output\")(x)\n",
    "\n",
    "    # Model Definition\n",
    "    model = tf.keras.Model(image_input, output, name=\"mushroom-model\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "# ====== Model Compile ======\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.Accuracy()],\n",
    "              )\n",
    "\n",
    "# optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# sparse_categorical_loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metrics= tf.keras.metrics.AUC()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_dataset:\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         output_logits = model(x, training=True)\n",
    "#         loss = sparse_categorical_loss()\n",
    "\n",
    "model.fit(train_dataset, epochs=10)\n",
    "model.save(\"mushroom_classifier.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "output_evaluation = model.evaluate(validation_dataset)\n",
    "output_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mlflow-Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mushroom_image_classifier\"\n",
    "try:\n",
    "    eid = mlflow.create_experiment(name=experiment_name)\n",
    "except:\n",
    "    experiment_obj = mlflow.get_experiment_by_name(name=experiment_name)\n",
    "    eid = experiment_obj.experiment_id\n",
    "    \n",
    "with mlflow.start_run(experiment_id=eid) as run:\n",
    "    mlflow.tensorflow.log_model(model, \"mashroom\")\n",
    "    run_id = run.info.run_id\n",
    "    print(\"run_id\", run_id)\n",
    "    print(\"exp_id\", eid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
